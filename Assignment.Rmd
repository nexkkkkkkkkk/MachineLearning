---
title: "PML Assignment"
author: "nexkkkkkkkkk"
date: "October 23, 2015"
output: html_document
---

## Introduction
The object of this assignment is to analyze data collected from accelerometers on the body of test subjects performing barbell lifts correctly and incorrectly 5 different ways, and develop a model that can predict, given the accelerometer data, the manner in which the exercise was performed. Data courtesy of HAR (Human Activity Recognition) http://groupware.les.inf.puc-rio.br/har.

## Method
### Loading and cleaning data
The original raw data consists of 160 variables, divided into 19622 observation for training and 20 for testing. The data was read in, and upon examination, it was found that many of the variables were derived statistics (such as average and skewness) which were not present in the test set (NA), hence those variables were removed for analysis purposes.  In addition, certain identification type variables that were not likely to be useful for prediction were also removed. The end result was 53 predictor variables, and the dependent variable "classe" with 5 factor levels (A-D) corresponding to the 5 different ways of performing the exercise.

```{r}
# load libraries
library(caret)
library(randomForest)

# download data
fileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(fileUrl,destfile = "pml-training.csv") 
fileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(fileUrl,destfile = "pml-testing.csv")

# read in data
train0 <- read.csv("pml-training.csv")
test0 <- read.csv("pml-testing.csv")

# remove statistic columns (all NA in test set):
# create an index variable ix which is the index of column names with stats in it
ix <-grep("(^av|^ku|^st|^va|^max|^min|^sk|^amp)",names(test0)) 

# Also remove the date/time related variables (including the "windows"), because they would 
# be only useful if we looked at predicting based on previous observations;
# also the observation number (X) is not useful for prediction, so remove that too.
# User name maybe useful in prediction, as individuals may exhibit different techniques, so
# leave that in
ix <- c(ix,1,3,4,5,6,7)  #adding "id" columns to be removed to the index

# create new train set with just the "good" variables
train1 <-train0[-ix]
#
# make sure the classe and user_name variables are factors
train1$user_name <- as.factor(train1$user_name)
train1$classe <- as.factor(train1$classe)

```

### Partition data for training and testing
When building a model, the resulting error rate is overly optimistic due to overfitting. In order to be able to estimate valid error rates, the data is split into two sets, one that we will use to create the model (train) and one that we will use to get a valid error estimates.  The "test"" data provided for assignment submission, with only 20 observations, is too small for good out-of-sample error estimates, plus it does not contain values for the dependent variable classe, so we will split the "train" data set into our train (tr) and test(ts) sets for model building and testing.

```{r}
#
# the test data provided (sample size 20) is too small to be useful in evaluating out of
# sample errors, so we'll separate out our own train and test data from train1 so that we
# can do cross validation
set.seed(12345)
inTrain <- createDataPartition(y=train1$classe,p=0.7,list=FALSE)
tr <- train1[inTrain,]
ts <- train1[-inTrain,]

```
### CART model
For classification type of predictive modeling, Classification and Regression Trees (CART) models are highly suitable to create decision-tree type models.  Using the caret library, we started with rpart, default parameters, on the tr training data set. The result was unaccapteable -- the final decision tree did not allow for the D possibility at all. Selecting the rpart2 model, which is the same as rpart but it uses maximum tree depth, did allow for D prediction, but it only predicted 56% (accuracy=.56) correctly based on the training set tr.  Expecting the error rate to be higher out of sample, it was not even worth calculating.

```{r}

#
# Trying a CART model
# Tried model "rpart" -- not a very good predictor, does not allow for D possibility; 
# refine with "rpart2" model which is the same except it uses the maximum tree depth.

set.seed(12345)
#rpart.fit <- train(classe~.,method="rpart",data=tr)
rpart.fit <- train(classe~.,method="rpart2",data=tr)
print(rpart.fit$finalModel)
plot(rpart.fit$finalModel,uniform=TRUE)
text(rpart.fit$finalModel,use.n=TRUE,all=TRUE,cex=.6)

# how good is this model? use confusion matrix to test predictive error 
#   on the training set tr
rpart.cmat <- confusionMatrix(predict(rpart.fit,tr),tr$classe)  
rpart.cmat$overall["Accuracy"]  #accuracy pretty bad
```

### Random Forest Model
Random Forest is a highly accurate decision tree based model that we applied next, hoping to improve on the dismal CART prediction error. Run on the training data set tr with the default settings, except for setting "importance=TRUE" to allow the importance of predictors to be assessed, the error rate has plummeted to less than 1%, and from the confusion matrix it can be seen that this model's predictions are pretty close to right on. Tuning a model that is already well fitting may further reduce the training set errors, but at the cost of increasing the test set errors (overfitting). Hence we did not attempt to improve on this "default" random forest model.

```{r}
#
# Trying Random Forest Model "rf"
set.seed(12345)
rf.fit <- randomForest(classe~.,data=tr,importance=TRUE)
print(rf.fit)
```

### Error estimation (cross-validation)
We would expect the out of sample error rate to be higher than the in sample error. The confusion matrix displayed with the rf model results did not, per se, show the accuracy (which is 1 minus the error rate), so we recalculated it (still based on the training tr data set) -- rounded to a 1, implying a perfect fit and a 0 error rate.  

Applying the model to the test data set, we would expect the error rate to be higher (and, by definition, accuracy lower). The resulting confusion matrix shows that the predictions are still pretty good, and the accuracy rate is 99% (implying a misclassification error rate of just 1%), just a bit lower than the in-sample-accuracy.  And that is considerably better than the CART model we originally tried.

```{r}

# how good is this model? use confusion matrix to test predictive error
#  --> first on the training set tr
rf.cmat.tr <- confusionMatrix(predict(rf.fit,tr),tr$classe)  #already shown in rf.fit 
rf.cmat.tr$overall["Accuracy"]    # a perfect 1!
#  --> then on the test set ts
rf.cmat.ts <- confusionMatrix(predict(rf.fit,newdata=ts),ts$classe)  
rf.cmat.ts$overall["Accuracy"]
print(rf.cmat.ts)
```

### Which variables are important?
Although the good predictive results from the random forest model imply that we need not fine tune it for this assignment, a simpler predictive model may be interesting for some other purposes.  The random forest model gives us the top 20 variables, rated by importance of variable predictivity potential.  To create a simpler model, you could try various models based on fewer variables, using the top ones shown on the plot.

```{r}
varImpPlot(rf.fit,cex=.75)

```

### Assignment test data set predictions
Predictions based on the test data set (20 observations) that came with the raw data:
```{r}
testCases <- predict(rf.fit, newdata=test0)
print(testCases)
```
## Conclusion
The basic random forest model (based on R defaults) produces good predictions with expected out of sample accuracy above 99%. This implies that you can make very good predictions with spot readings of accelerometers, and using time windows (part of the raw data) is not necessary.

